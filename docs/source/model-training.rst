5. Model Training
==========
AI and ML are iterative methods that uncover valuable information and insights that are not clearly apparent from within data. The process helps unlock the data’s full potential to deliver business value decision support and automation. AI-ML methods use models that training algorithms build from relevant data to fulfil the desired tasks and objectives, and, therefore, data are at the core of AI-ML, underpinning its operation and success. Without relevant data of the required quantity, quality and frequency, AI-ML would yield inaccurate and potentially unusable output. It is important to remark that despite the fact that scientists worldwide are developing more dynamic and adaptive methods, the reality remains that different business needs are likely to require the use of different AI and ML methods depending on the user requirements. More importantly, the models built by the algorithms need specific data to address the requirements at hand. Similar to the human brain, if we would like an algorithm to predict traffic jams on a particular road, then a sufficient quantity of relevant historical data about previous traffic jams, dates, times and weather conditions will significantly help in making more accurate predictions. However, the data required to predict a traffic jam are different from the data required to predict a football match outcome, even if we use the same methods. 

5.1 Training Algorithms
---------
In this section, we introduce the notion of training algorithms and provide an overview of the gradient descent (GD) approach used by algorithms that train supervised ML models. This discussion will require some simple mathematical notation. For simplicity, we will refer to an ML model for classification, defined by a function Fw : DS C from the input data space (DS) to a finite set of classes or categories (C). As an example, the inputs to Fw could be sensor readings about the rotatory engines presented in Section 1, and the outputs could be the (IMMINENT and NOT-IMMINENT) labels about failure of the running example of Section 1. 

Here we use the mathematical notation Fw to designate the ML model map-ping inputs to one of the categories. This notation is useful because the subscript in Fw highlights the array w of the ML classifier’s internal parameters, often called its weights. Of course, the data-flow structure of the computation performed by Fw depends on the specific ML model used, but the result of the computation depends on the values of the weights w. For example, using a multi-stage neural network (NN) as our classifier, the output of each stage of the network is computed as a weighted combination of the outputs coming from the previous stage, called activations [#f7]_ . Activations to the first stage coincide with the one-hot encoded inputs discussed in the previous sections. 

In essence, model training algorithms are the algorithms that adjust the weights w of the model so that Fw coincides over S with a target function f: DS → C, which expresses the correct classification of all points in the input space. 

In terms of our notation, the training set mentioned in Chapter 1 is a set S of sensor inputs for which the values of f : DS → C (i.e., the right IMMINENT and NOT-IMMINENT labels), are known. In the remainder of the section, we outline how training algorithms work. 

.. rubric:: Footnotes

.. [#f7] The reader interested in a general introduction to NNs can consult Michael Nielsen’s free online book (http://neuralnetworksanddeeplearning.com/). 

5.1.1 Gradient Descent Training
~~~~~~~~~~~~
GD is a popular family of iterative algorithms for training supervised ML models like neural networks. GD is based on a simple mathematical notion, which can be expressed as follows: in any smoothly changing (in mathematical terms, differentiable) function f, a maximum or minimum is always where the function flattens out (i.e., where the function graph’s slope is zero). Calculus highlights that a function of a single variable f(x) flattens out where its derivative f i, expressing its slope, is zero [#f8]_ . For multidimensional functions f (v), where v is an array of variables, we can look for points where the gradient ∇ f, the multidimensional analogy of the derivative, becomes zero. The basic version of GD works as follows: at each step, the GD algorithm perturbs the ML model’s weights vector w, applies the model Fw to one or more elements of the training set S and computes the model’s current classification error Ew (i.e., the difference between the outputs of Fw on those elements and the elements’ labels). Then, GD uses the error’s variations across these input elements to nu- merically estimate ∇Ew and updates w based on it. The model’s classification error Ew can be computed as the linear (L1) or quadratic (L2) sum of the differences between Fw outputs and the ground truths available as the known labels of the elements of the training S, in our notation f (S). 

Informally, it can be said that by this procedure, the GD tries to ‘drive’ Fw toward smaller values of the error gradient, progressively reducing Ew. The final goal of GD is to find the point where the ∇Ew gradient is zero, corresponding to the vector w that minimises Ew on the training set S. 

.. rubric:: Footnotes

.. [#f8] The reader interested in a general introduction to NNs can consult Michael Nielsen’s free online book (http://neuralnetworksanddeeplearning.com/). 

5.1.2 A Closer Look 
~~~~~~~~~~~~
While the above informal description can be sufficient for a general understanding of GD-based model training, from the more mathematically-aware data manager’s point of view, it is also useful to take a closer look to the computation performed by the software implementations of GD to estimate ∇Ew. This requires just a little bit of algebra. At each computation step, given the current weights vector w, the GD algorithm generates three nearby vectors w1 , w2 , w3 . This way, computing Ew(w)Ew(wi) wwi gives approximately the directional derivative of the error Ew at w in the direction wwi. The derivative is indeed the projection of the gradient ∇Ew(w) in the direction of w − wi, or ∇EwEwwi Now, let us assume the following approximation holds:
Ew(w)Ew(wi) = ∇Ew(w)(wwi). (2) 
wwi 
As the error Ew is itself a scalar, i.e. a single number rather than an array, this is a system of three linear scalar equations in three unknowns (the components of ∇Ew). Basic algebra tells us that, if the three vectors wwi 

As the error Ew is itself a scalar, i.e. a single number rather than an array, this is a system of three linear scalar equations in three unknowns (the components of ∇Ew). Basic algebra tells us that, if the three vectors wwi are orthogonal, this linear system has a unique solution, so it can be solved numerically by the GD algorithm to obtain the gradient’s components. 

This computation requires calculating Ew, a computation that can in principle be done using a single element of S. However, the different implementations of the GD algorithm used in ML software libraries differ form one another in terms of the number of elements of the training set S that are used at each step to compute Ew. As intuition suggests, the higher this number, the higher are both the fidelity of the GD algorithm in following the error gradient and - unfortunately - its overall computation time. 

*	Stochastic Gradient Descent (SGD), is a variation of the GD approach that computes Ew, estimates ∇Ew and updates Fw using a single random entry e of S. Frequent updates of Fw introduce a noise-like “jerky” effect on Ew, but allow for continuously monitoring the ML model’s performance. 
*	Batch Gradient Descent (BGD) computes error Ew (and estimates ∇Ew) for each e ∈S, but only updates Fw after having scanned all of S (i.e. once for each epoch). Our intuition suggests that BGD’s lower frequency of updates results in less sign variations in Ew. For our purposes, it is worth remarking that - due to the granularity of ∇Ew estimates - BGD is usually implemented in such a way that all the training set S needs to be in memory at the same time. 
*	Mini-Batch Gradient Descent (MBGD) splits randomly f into subsets (the “small batches”), which are used to compute Ew, estimate ∇Ew and update Fw accordingly. In this case what is used to estimate ∇Ew is actually an aggregation hMB(Ew), where MB is the mini-batch. Instead of computing the aggregation has a sum of errors over the mini-batch, it is common practice of implementations to take the average, to keep Ew variance under control. 

Today, the MBGD variant of GD has become increasingly popular and widely used for training “deep” ML models. Its update frequency is higher than the one of plain BGD; also, its batch size (one of the algorithm’s hyper-parameters) acts as a control over the learning process. Small batch size values may give faster convergence at the cost of introducing noise in the training process. Large values give a learning process that converges slowly but provides accurate estimates of Ew gradient. 

5.1.3 Federated Learning
~~~~~~~~~
The variations of the GD training algorithm described above are all centralized: all the training set S is in a single place and all of it is considered for extracting batched for the gradient’s computation. In principle, the GD algorithm can be made parallel by using multiple batches B at the same time, and training the the ML model on multiple processors. Parallel implementations of GD should not be confused with federated learning, which is targeted to addressing data privacy and security as well as data access rights. 

Federated learning is based on a different notion: multiple nodes hold each a part of the training data S, without sharing it. In terms of our notation, the training set S is partitioned into multiple local training sets S1,....Sn held by their respective owners, without explicitly exchanging data samples. The general principle of federated learning consists in training local models on local data samples and exchanging the models’ internal parameters (e.g. their weights) at some frequency, in order to generate a global ML model shared by all nodes. In federated learning, the local training set’s parts S1,....Sn are typically heterogeneous and their sizes may span several orders of magnitude. Moreover, the partners involved in federated learning may be unreliable as they are subject to more failures or drop out. There are two major families of federated learning algorithms. 

*	Centralized federated learning In centralized federated learning algorithms, a central coordinating node orchestrates the different steps of the training algorithm and coordinates the other participating nodes during the algorithm’s execution. The coordinator is responsible for the nodes selection at the start of the training process and for aggregating of the received model updates. 
*	Decentralized federated learning In decentralized federated learning algorithms, participating nodes collaborate in a peer-to-peer fashion to obtain a global ML model. This organization aims to prevent single-point failure. 


5.2 Automatic Organisation of Data
---------
AI/ML models help to better understand data and uncover patterns and information hidden within it, to provide additional valuable insight. Hence, it is no surprise that one of the key challenges we first encounter when dealing with data, both structured (numerical or categorical data) and unstructured (text data), is the need to group together similar objects that the data represents. These groups will contain the objects that are more similar to each other than those in other groups based on some attributes of the objects. In many cases, the user does not have a view of the groups themselves or indeed the number of distinct groups. Hence, clustering the objects that the data represents provides an initial understanding of the data, that will help with further analysis. For example, let us consider a call centre for a retail bank, that receives a large volume of calls from customers. If the bank’s call centre manager is planning training topics for his employees, then grouping the calls together in groups based on similarity will show the topics that are generating calls, and the volume of calls associated with each topic. This insight will help the bank focus the training on areas of importance to the customers, and help to provide a better service. This kind of grouping or clustering could also uncover topics that the manager may not have previously anticipated. In AI/ML, a clustering algorithm is a technique or method used to automatically group the objects that the data represents into different clusters based on their similarities. This is known as unsupervised learning. 

5.3 Generating New Data
---------
It is possible to generate brand new data using certain AI techniques. These techniques are able to produce augmented data, i.e. synthetic training data of any size, targeting applications where requirements and results depend on greater quantity. You can use synthetic data when you are required to train an ML model requiring a larger amount of data than what you have, or to cover a different set of data points that have been too difficult to obtain by normal means. Synthetic data can be produced from any type of data including numbers, text, images and sounds. Other than for training purposes, new data requiring creative thinking can be produced in this way. Creating new art pieces, music or writings is possible using ML models that can learn the patterns from data made from the same creator. 
