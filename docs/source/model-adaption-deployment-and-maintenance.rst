7. Model Adaption Deployment and Maintenance
===========
7.1 Model Deployment
-----------
Imagine that you developed an ML model which can forecast the likelihood of a particular outcome with a certain confidence score. It is a good step, but the process is not finished yet. In a perfect world, you want your model to evaluate real-time cases to be able to make decisions accordingly. This is where model deployment comes in. The deployment and maintenance of ML models is one of the most critical challenges that organisations face today. Numerous data science projects fail to get into production because of the many strains in the deployment phase which obstruct the entire process. These challenges originated in the dynamicity and complexity of the environment where heterogeneous and diversified components interact with one another in such contexts. 

For applying ML in practice, it is a critical task to evaluate and adapt the existing, established software engineering practices that ML literature has thus far not taken into proper consideration. Actually, ML deployment is a topic that is completely unrelated to data analysis, model selection and model evaluation; therefore, it is not properly received by those without a proper background in software engineering. Recently, the interest in establishing substantial methods and practices in the development of ML systems has started to grow. The deployment of an ML model is referred to as the process of integrating an ML model with an existing production environment in which an input is processed to generate an output. The purpose of deploying a model is to enhance the ability to make predictions through the use of the model which is already trained and made available to different systems. The ML deployment is strongly dependent on the architecture of the overall system, the set of iterations and configurations of its software components. There are a couple of requirements that need to be satisfied before a model may be considered ready for deployment: 

* Portability deals with the ability of the model, in terms of software and hardware, to be transferred from one system to another. This peculiarity will be important in evaluating where the system will be deployed. 
* Scalability has to do with the system’s ability to grow over time. A model is said to be scalable when it does not require modifications and redesign to maintain performance unaltered. 

A similar parallelism exists with cloud workloads, where companies must decide to deploy on premise, in the cloud or adopt a hybrid approach; the same decision must be made for ML algorithms. It is not an easy choice, as the physical location where ML algorithms are trained and deployed can affect their performances as much as the algorithm itself. 

7.1.1 Cloud-based Deployment
~~~~~~~~~~~
The use of the cloud for the deployment of ML models is the most common method. Cloud platforms offer a range of services that developers and data scientists can use to design, train and deploy ML models. Furthermore, having this support available allows for the transition to an open and flexible environment where data can be rapidly collected from the cloud storage, prepared and injected directly into the model. The downside of this approach is that it could be extremely demanding to move the data from where they are generated into the cloud storage where they are used for developing and training the model. Sometimes it is difficult if not impossible to move large amounts of data in a centralised repository due to high latency, bandwidth limitations and excessive costs. In modern contexts where an increasing number of devices are connected to each other and hundreds of gigabytes – if not terabytes – are generated per day, those issues represent a real challenge that can force the developers to make unwanted compromises. In fact, the transferring into the cloud of a selected data ’sample’ for the model training and maintenance is often the only alternative when the transfer of a large amount to a centralised data centre is not a reasonable choice. As a result, the efficiency in obtaining reliable information from newly generated data in real time is restrained and the ability to analyse a combined data set comprehensive of newly generated and historical data is diminished. Furthermore, the moving of data across jurisdictions can increase privacy and geopolitical concerns. 

7.1.2 Edge-based deployment
~~~~~~~~~~~~
It is estimated that the daily amount of data generated from personal or enterprise IoT devices is around 250 petabytes, and as mentioned in previous sections, the latency of the network can have a severe impact, especially for IoT devices. As an example, autonomous vehicles need to collect and analyse a massive amount of data from their own sensors and from the nearby devices. If the reaction time of the vehicle depends on the response time from the computing core of the network, every minimum delay could cost lives. For those specific cases it is mandatory to adopt a different approach. Edge computing lends itself perfectly to managing these data as it provides a sufficient incentive to collect and process data on the devices themselves rather than in the cloud or in a remote data centre. In short, the key benefits for this approach are two: real-time analysis of data and reduced data transmission to the cloud. As a result, IoT devices are less affected by latency and have faster reaction to status changes. In this context, edge computing can provide predictive analytics on the edge devices. 

One of the options for businesses is to design and train ML models in the cloud and upload the algorithm onto the device to execute close to where data are generated. This method guarantees the flexibility and simplicity of developing in the cloud and the efficiency of running directly in the proximity of the source data. 

A second option is the development and training of the ML method on the device itself, using the collected data passing through. The system relies on intelligent edge devices providing the required functionalities and interfaces for development. 

These approaches partially solve the problem of latency, but they also present accuracy problems. Algorithms developed on the cloud and brought to the edge are based solely on data samples, while developing accurate ML models only on the edge could be burdensome because those devices are designed and optimised to work with minimal re- sources and low power; moreover, the amount of data they can store for analytics can be tight. 

A possible solution could be the hybrid approach, allocating computing resources at the edge. Those resources will reside in the near proximity of the devices but not within; this will ease the burden of transferring data from a centralised system. The data analysis could be performed in a distributed way by installing computing resources and storage in the proximity of the devices, such as schools, hospitals, banks and others. This approach offers the most complete solution, allowing the analysis of a nearly unlimited amount of data of any age and without restrictions against crossing different geopolitical areas. It follows that this model can bring the advantages of big data directly to edge computing, and, at the same time, it allows the application of ML algorithms on distributed data on all edge-devices in parallel. 



7.2 Model Maintenance
-----------
Data are the most important part of the ML model. Once the model fits perfectly into the available data set and provides accurate predictions, it is essential to ensure that the system continues to do so over time, with the most up-to-date data. As an example, with an ML model for predicting the real estate market, house prices are read frequently over time, so considering a model that was trained one year earlier could provide very inaccurate predictions when used for current market data. In this case, it is imperative to have up-to-date information for the new training. When designing an ML model, it is important to understand how and how often the data will change over time; a carefully designed system takes this into account before development to ensure an easy and smooth model upgrade. 

7.2.1 Model Drift
~~~~~~~~
An ML model that is running using static data (i.e., data whose statistical characteristics do not change over time), does not suffer a loss in performance because the data that are used for predictions belong to the same distribution as the data used for training. Unfortunately, in most real-world cases, the model exists in a dynamic environment and so is subject to revision. In this scenario, a concept drift is the performance decay of an ML model; at the origin of this well-known concern is a change in the environment that breaches the initial hypothesis of the model rather than a contraction in the capabilities of the model itself. A model degrades over time due to various factors and variables, depending exclusively on the context in which the algorithm works. The model performances decrease over time, and, with a certain rate, both are difficult to predict in advance. For this reason, it is essential to keep all these factors in mind when diagnosing the problem and determining the most effective method for retraining the model. It is important to understand how to track the drift. There are several approaches, but, depend- ing on the data and the prediction algorithm, not all the solutions may be suitable for a specific case. The most intuitive way to identify drift is to explicitly determine that the model performance is impaired and to try to quantify the deterioration. Measuring model accuracy on live data can be problematic, as it is necessary to access both the predictions generated and the ground truth values (i.e., the information generated by direct observation), but predictions may not be stored or those observations may not be available. 

Another way to address the problem is to infer the drifting. Because a decay of the model is expected due to the deviation of the serving data with respect to the training model, a comparison between these two distributions can give an estimate of the drift. This solution is particularly useful when it is not possible to extrapolate the ground truth from direct observation due to the nature of the data generated. 

7.2.2 Model Retraining
~~~~~~~~~~~~~
We have defined the model drift and how to recognise it; now we need to understand what the next step is. Usually, a model that has been deployed for production should be the result of a rigorous validation process; the resulting algorithm is the best prediction method for a given type of data. Since the performance in predictions decays due to a variation of target data, the model’s retrain- ing should not involve any changes in the model generation process. In fact, it is simply a matter of relaunching the process that generated the first instance, but on a new training set. The solution is new training per- formed on a new data set that reflects the evolution of the environment and the current reality. At this point, it is necessary to understand when to train and which new data set to use. The problem itself could directly provide an answer to the previous questions. Suppose you want to develop a model that generates predictions about the university courses students enrol in. This model can be run on students currently attending the last year of high school in order to prepare the entrance tests for the various universities. This kind of prediction must be done annually; it would not make sense to repeat the process more frequently as the data available for a new training would not exist. Therefore, we can only decide to retrain our model at the beginning of the academic year when we have the new enrolment data. This is an example of periodic retraining. In general, sudden changes in training data require retraining often, even daily or more. Slower variations will require monthly or even annual training. Following are a few approaches regarding methods that can be adopted in different contexts: 

*	ONE-OFF. This method is used when a continuous retraining of the model is not required but can be done periodically. In this case the model is put back into production once an ad-hoc training is carried out and the model stays in place until becomes obsolete again. 
*	BATCH. This method allows you to have a constantly updated version of the model. The model is updated with a subset of the data at a time without necessarily using the entire set at each update. This method can be used when the model is frequently used but does not necessarily require real-time responses. 
*	ONLINE (real-time): Real-time training is possible with online ML models. The model is trained at every data set submission and is expected to provide a prediction for this data set in (near) real time. 

The privileged option is to have an automatic drift management mechanism in case there is availability of technologies and infrastructures for monitoring the parameters discussed in the previous section. This operation requires continuous monitoring and a mechanism for triggering the training process whenever the diagnostics on the active data diverge from those of the training data. Obviously, the challenge is to determine the threshold value for the divergence between the two sets. If this value is too low, there is a risk of having too frequent retraining without benefits and with high costs. On the other hand, if the value is too high, the risk of delaying the retraining, which results in a model that does not perform in production, may increase. A further intrinsic problem is in determining the correct amount of training data to be supplied to faithfully represent the changes in the environment being observed. In fact, even if the world has changed, it could be counterproductive to replace the previous training data set with a considerably smaller one in the absence of additional data. 



7.3 Data Disclosure Risks and Differential Privacy in Model Deployment
-----------

We now informally discuss some data disclosure risks that arise when out-sourcing ML models, e.g. by deploying them at the premises of a service provider, who could gain information from the input data or guess the information originally used for training the ML model. To better understand the notion of training set disclosure, we go back to the simple example and mathematical notation we used to describe training: a model :math:`F_w` trained on a training set S for classifying the items of a data space :math:`DS` into classes of interest belonging to a set :math:`C`. This deployment procedure involves a disclosure risk whenever :math:`S` can be inferred from :math:`F_w` outputs. Disclosure will happen if by running or observing :math:`F` in production, an attacker can reconstruct one or more entries of the training set :math:`S`. 

One could be tempted to require that computing :math:`F` in production (i.e., performing the inference) should reveal absolutely nothing about the training set :math:`f`. This is unfortunately just a re-phrasing of the classic *Dalenius requirement* for statistical databases, which cannot be fully achieved if enough side information about :math:`S` is available. However, Cynthia Dwork proposed more than a decade ago the notion of *differential privacy*, which, intuitively, captures the disclosure risk. Dwork's seminal work has turned the "impossible" Dalenius requirement into an achievable goal: observing the execution of :math:`F_w`, the service provider should be able to infer the same information about an entry :math:`e \in f` as by observing :math:`F_w`, obtained using the training set :math:`S - \{e\} + \{r\}`, where :math:`r` is a random entry. This will provide the owner of :math:`e` - assuming she has something to gain by knowing the result of :math:`F` - with some rational motivation for contributing :math:`e` to the training set, as she will be able to deny any specific claim on the value of :math:`e` that anyone could put forward based on :math:`F` (a notion called *plausible deniability*). The most investigated approach to achieving differential privacy consists in introducing a degree of randomization in the computation of F, making :math:`[F(x)]` a random variable over :math:`DS`. Techniques vary on how and where to inject such randomization, depending on the nature of :math:`F_w`.

Often, random noise is simply added to the training set, with zero average and a standard deviation :math:`\sigma = \frac{1}{\varepsilon n}`.

While the discussion above provides a general idea of data randomization to prevent disclosure, some additional remarks may be of interest for the mathematically-aware data manager. The probability density often used for such noise is the Laplace distribution: 

.. math::

   p(z) = e^{-\frac{|z|}{\sigma}} = e^{-|z| \varepsilon}   (3)

The distribution of this random variable is "concentrated around the truth": the probability that :math:`[F_w]` is :math:`z` units from the true value drops off exponentially with :math:`\varepsilon z`. This randomization introduces uncertainty, as the provider no longer computes :math:`F_w` but the value of a random variable :math:`[F_w]` with Laplace distribution whose average coincides with :math:`F`. 
