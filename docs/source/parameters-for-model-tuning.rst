6. Parameters for Model Tuning
===========
There are certain parameters which define high level concepts relating to ML models, such as their learning function or modality, and cannot be learned from input data. These model parameters, often called hyper-parameters, need to be set manually, although they can be tuned automatically by searching the model parameters’ space. This search, called hyper-parameter optimization, is often performed using classic optimization techniques like Grid Search, but Random Search and Bayesian optimization can be used. It is important to remark that the Model Tuning stage uses a special data set (often called validation set), distinct from the training and test sets used in the previous stages. An evaluation phase can also be considered to estimate how the model would behave in extreme conditions, for example, by using wrong/unsafe data sets. 

.. admonition:: Business Goal Definition Phase in a Nutshell

   AI Model Tuning in a Nutshell: Apply model adaptation to the hyper parameters of the trained AI model using a validation data set, according to deployment condition. 
   AI Model Tuning in our Running Example: ACME data scientists run the 2D RNN model trained for fault prediction on an additional validation dataset and choose the best values h, k for the RNN’s tensor dimensions. 


6.1 Model Hyper-Parameters
-----------
Model hyperparameters represent a higher-level set of variables controlling model design and architecture choices as opposed to the learnt data- or feature-level parameters capturing the content of the data and their characteristics and structure. Hyperparameters typically determine mathematical representation, apparatus and operation logic as well as various algorithm-level choices that decide about the complexity, efficiency, breadth and depth of the optimisation process underpinning the model training and testing. 

Hyperparameters always accompany virtually every ML model, although the extent to which the model implementation software exposes their control to the user varies enormously, in part due to the model’s intrinsic complexity and in part to different software engineering strategies. The drive towards AI adoption increasingly sets the tendency to automate setting of the strongly correlated choices to limit their ambiguity and uncertainty for user convenience. From the user’s perspective, hyperparameters may not even be visible at first, as the models are always preset with the most common default hyper-parameter choices. This is both an advantage and a risk in practical applications; on the one hand, such a model build can be instantly executed without much knowledge about it, but on the other hand, great models with the wrong hyperparameters can be prematurely discarded based on preliminary, ballpark performance estimates. 

ML models vary significantly with respect to the number of hyperparameters and the sensitivity of the model’s performance to their choices. Traditionally, ML models evolved from virtually fixed hyperparameterless, stable settings like in linear regression; to a setting with no more than a few hyperparameters like in k nearest naughbour (KNN), decision trees, naive Bayes or Gaussian mixture; and finally up to larger and larger numbers of hyperparameters in layered composite network models, where each layer is effectively controlled by its own set of hyperparameters. With this evolution, an individual ML model de facto becomes an ever-growing family of model version choices – hence its growing sensitivity of predictive performance on ever more expanding, subtle sets of model hyperparameters. 

In general, models with a small number of hyperparameters are preferred, as long as their predictive power and application flexibility are not compromised. Sometimes mathematical ingenuity can be used to eliminate hyperparameters risk-free, as in the case of Gaussian process generalising multivariate normal distribution-based models. However, this is usually achieved at the expense of significantly grown computational complexity, with only marginal improvements in predictive performance. The practical strategy that emerges from the compromise of these colliding tendencies of exposing the hyperparameters to model fine-tuning is that the ML model release, however complex, has identified several key hyperparameters that account for the vast majority of its modelling and predictive capabilities, and the user is presented with the option to optimise the model along with these hyperparameters if required. 


6.2 Hyperparameters Optimisation Strategies
-----------
Each configuration of hyper-parameter values set to train and test the ML model represent just a single solution evaluated by the model testing error obtained through the cross-validation or other the generalization error estimate method. Since the whole cycle of model training and testing is typically a costly operation, optimization of the hyper-parameters is rarely a search for an optimal set of hyper-parameters’ values that yield the best predictive performance, but for practical reasons often to quickly find a good and stable set of the hyper- parameters’ values that consistently return good predictive performance of the model. Depending on the cost and time of an individual model build hyper-parameter optimization can be, and typically is, aided by the parallelized evaluation process but what is the most characteristic about this optimization is a very careful selection of the hyper-parameters’ values before passing them for model build and the evaluation. For this reason among the most common optimization of the hyper-parameters are the grid search, probabilistic Bayesian and the greedy-linear iterative methods. 

6.2.1 Grid search method
~~~~~~~~~~~
Grid search method’s principle is to cover the whole domain of every parameter with a regular grid of a couple of values and then evaluate all the combinations of such grid. Given n hyper-parameters to optimize and a grid of size k, there are however still kn evaluations required to complete such grid search. The grid size can obviously be reduced if necessary, some conflicting or impossible combinations may be manually excluded from the evaluation to speed up the search. The real problem, though, is that the grid search does not in general exploit the previous iterations’ to improve the next iterations’ performance. For this reason, given the growing cost of each model evaluation, the iterative probabilistic or greedy search methods are more commonly used in practical applications. 

6.2.2 Bayesian hyper-parameters optimization
~~~~~~~~~~~
Bayesian optimisation of the hyperparameters tries to build a simple probabilistic model of the relationship between the hyperparameter values and the predictive model performance and then uses it to improve the selection of the next parameter set based on all the previous set evaluations. Each new evaluation yields more reconciliation evidence between the probabilistic distribution model and reality and allows experts to rather quickly find the model-inferred near-optimal choices. The Bayesian hyperparameter optimisation method is very quick. However, its performance still depends on the accuracy of the probability distribution assumptions, which must still be made manually for each parameter (although Gaussian is most commonly assumed by default). 

6.2.3 Greedy-linear iterative search
~~~~~~~~~~~
Greedy-linear iterative search is a hybrid search that combines the advantages of the grid and Bayesian searches. Starting from the default hyperparameter values, it carries out the search sequentially, optimising single parameters one at a time with other parameters fixed. Once the individual hyperparameter is optimised, its value becomes fixed and the next hyperparameter is optimised the same way. These rounds of single-parameter optimisations continue in a loop until an entire round occurs without a single parameter change. Note that for individual parameter optimisation there is the freedom to use a grid search, Bayesian or any other search. The search may therefore flexibly incorporate various desired elements of other optimisation techniques, and, since it typically completes within just a few rounds, it can be considered near-linearly complex with respect to the number of hyperparameters to optimise. 


6.3 Transfer Learning
-----------
ML methods have proven to be useful in analysing a vast amount of data in its various formats to identify patterns, detect trends, gain insight and predict outcomes based on historical data. However, ML models are challenging to reuse from a domain due to the change in the data distribution. Moreover, training ML models with good accuracy requires a massive amount of labelled data, which is expensive and time-consuming. New approaches were developed that can reuse and adapt existing model(s). These techniques fall under the collective name of transfer learning (TL). More specifically, TL is a methodology to transfer knowledge learned from one model to another. For example, in the case of a model trained to detect a specific type of object from images, the knowledge it has gained can be transferred, via TL, to another model that detects a different kind of object from images. TL has achieved excellent results in many domains, including image processing and NLP. When reusing ML models, understanding data distribution between the two domains is essential. The difference between the distributions may result in lowering the model accuracy. 
