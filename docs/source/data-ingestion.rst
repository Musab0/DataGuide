2. Data Ingestion
=====

2.1 Sources of Data
-------
In recent years, businesses and organisations have been moving more and more processes and services online. These digital processes generate a huge amount of data every second. We can group these data into three distinct categories:

#. Sensor-generated data (i.e., Internet of Things [IoT] data). These data are generated by equipment without human involvement, such as smart meters, road sensors, street cameras, satellites and many more sources. This type of data source extends our capability of sensing and monitoring the world around us, which, in turn, helps us in automatic decision-making through the analysis of these data. Self-driving cars are a good example of such systems, as they automatically analyse data to navigate their environment and drive autonomously.

#. User-generated data. Online user-generated data include informal content posted by individuals on social media platforms such as Twitter, Facebook, Instagram, YouTube, forums, blogs and other mediums of communication, as well as more formal content in newspapers, news agencies, government media outlets, public information websites and so forth. It is worth noting that social media outlets such as Twitter, Instagram and Facebook are also being used by government and news agencies as a formal mode of information sharing. This kind of data provides invaluable insights into people’s and communities’ views, per- ception and behaviour, as well as what is happening in the world and the impact on opinions, needs and actions.

#. Transaction data. Such data are generated from human actions (e.g., invoices, payment orders, storage records, delivery receipts, ordering a passport online). Analysis of this type of data together with other sup- porting types provides a more comprehensive situational awareness and highlights the consequences of events that are taking place. For example, the COVID-19 pandemic results in lower transactions in terms of physical shopping and increased online shopping. COVID-19 has also resulted in certain goods like masks being more in demand than fuel.

2.2 Data Collection/Ingestion
-------
Data are key to AI applications and hence data collection is an important stage of the AI application life cycle. There are multiple types of data that can be obtained from multiple sources, as previously described. When data from multiple sources are used, they can be composed as a vector, which is a multidimensional data point. 
Data can be received in real time as a data stream or imported in batches at different time points. Receiving data as batches can be in the form of macro-batches (large data chunks) or micro-batches (small data chunks). Data can naturally be imported using one or multiple methods, depending on the data systems used/available, user requirements and the AI application being developed. 

In terms of different types of data sources, data ingestion methods vary. For online content and data, harvesters (scripts) are needed to automatically grab the content from websites or social media platforms, either directly from the websites or through application programming interfaces (APIs). Such data-collection methods tend to run periodically to import data in micro- or macro-batches. There are many commercial and open-source products that can help obtain the relevant content from websites. In addition, a number of platforms and websites provide more structured and consistent methods to get the relevant data; examples include RSS feeds for news agency websites and APIs for social media platforms. Thanks to the advances and maturity of sensors and communications technologies, almost all IoT sensors have built-in capabilities to push sensor data in predefined formats to storage systems for easy access and analysis, as presented in Section 2.3. Depending on the application requirements, sensor type and communication capabilities and costs, data can be provided as live stream or in batches. In some rare cases, such as for legacy sensors, or for security reasons, data must be extracted programmatically for the sensors and/ or stored locally; hence, specific steps will be needed to extract the data, depending on the use case and type of technology. 

For transactional data, the hardware and/or software systems used (e.g., ATMs, point-of-sales terminals, online shopping websites) have systems to automatically log all transactions and related data, such as time, date and type of transaction, into storage databases. Collection/ingestion of this type of data is automated by its devices/systems, and no further effort is needed. There are certain constraints associated with data collection/ ingestion and usage, drawn from rules, regulations, best practices and potential business and technical matters. Data ownership and its intended use and time of use are often subject to laws and regulations which can also vary by data type. Certain data cannot be used for certain purposes or within or outside a particular time frame. Even if data ownership is clear and the data are not regulated by laws, there are best practices restricting their use (see next section). An entity may have access to health data that can be used for certain applications (e.g., diagnosis) when the data owner (the patient) has agreed or consented. On the other hand, a company might have data regarding the sales of a specific product or their market performance that, though not regulated by law, they choose to use internally but not share outside their organisation. Last but not least, in some cases automatic data collection/ingestion can sometimes be affected by technical issues, such as sensor failure or communications failure, that can impact the quality of data. There are methods and techniques that can help address some or all of these issues (usually done in the data preprocessing stage in the AI-ML life cycle, Section 4). 

Additionally, and depending on user requirements, it is possible to collect supplementary data describing the actual data for context. Such data are known as metadata and usually provide additional insight on the data, enabling more effective analysis (Section 2.3). 

2.2.1 Dataprivacyandanonymization
~~~~~~~~~~~
One of the major causes for resistance to data sharing is the risk associated with violating data privacy. Data, in many cases, contains private personal information that should not be made available to the general public, as it might cause harm to the concerned individuals. Simpler approaches such as masking the data might be appealing due to their low complexity. This might include substituting social security numbers with pseudo-random identifiers to hide the private information, as shown in Figure 10. 
However, this approach does not translate well to AI-ML applications, as it destroys many of the mathematical properties that are needed. To facilitate joint privacy-preserving analysis, researchers, academics and private companies have put forth a lot of effort to come up with intelligent and advanced privacy-preserving data-sharing schemes. These data-sharing schemes promise to provide the ability for two or more entities to securely and privately share data to carry out collaborative analytics, without revealing any private information to each other. In recent years, these techniques have matured significantly and now come with strong anonymisation guarantees, while enabling more advanced forms of analytics. Currently, there are three main branches that address data anonymisation which are: 

* Homomorphic encryption. A class of encryption algorithms that allow for certain mathematical operations to be carried over encrypted data without the need for decryption 
* Secure multi-party computation. A cryptographic technique that allows two or more entities to jointly perform computations on data without revealing the data to each other 
* Differential privacy. A model to aggregate data such that no identifying data on any individual is available 

To successfully and correctly apply any of the above anonymisation techniques, it is paramount to understand the features of the data set, anonymisation scheme and analytics to be performed – and then make the decisions based on those factors. However, the general principle is that the stronger the anonymisation technique, the less accurate and more time-consuming the analytics become. 

2.3 Data Storage
-------
The field of knowledge representation in AI deals with representing the knowledge used and produced by AI models in such a way as to solve complex problems, like communicating with humans using natural spoken or written language. In turn, data representation is the time-honoured computer science field dealing with the different formats for storing and accessing data sets, such as the ones used to feed AI models in the different stages of the AI life cycle. The connection between these two fields is represented by metadata, which provides additional information about the input data to be fed to AI models. The types of data that need to be represented in AI include facts (e.g., trustworthy database records), events (e.g., sensor data) and meta-knowledge (e.g., metadata describing how, when and by whom other data were collected). Managing these data types in AI data storage requires the management of a series of intersecting data representation standards. These include:

#. data structure standards, 
#. content value standards, 
#. communication standards, 
#. syntax standards. 

This section describes some of the available standards for data management from an AI data-usage perspective, in an effort to provide a landscape of data representation standards for AI storage. 

2.4 AI Data Storage
-------
ML and deep learning rely heavily on the availability of massive data for training purposes. For all AI applications, it is critical to have a standard data infrastructure (AI storage) that is scalable and can apply the FAIR data principles (findable, accessible, interoperable, and reusable) among heterogeneous data sets from various domains. Data storage for AI aims to host or collect quality data of different types and from multiple sources to create an integrated data storage. The purpose of AI data storage is also to set up an environment where AI model designers can easily judge, collect and utilise data. Besides data providers and users, AI storage may also have interfaces for other players, including data distributors that provide mediation between data providers and users. 

By making data representations interoperable in the data storage layer, data scientists and other users can focus on the substance of the AI problem they are trying to solve. This allows them to quickly unlock insights and benefits from data analysis. Besides interoperability, a major goal of data representation is protecting data confidentiality and personal privacy. A technology roadmap for AI data governance and management is therefore critical to any enterprise or organisation wishing to adopt AI. The road map needs to express the overall direction of how to manage data generated from the organisation’s products and services. 

2.4.1 AI Data Formats
~~~~~~~~~~~
The data formats used with AI algorithms are not necessarily unique and can be found in other applications, as well. However, some of the formats are more commonly used than others. The following are the most common: 

*	The single value representing an integer, a float or a string 
*	An array of single values, all of them having the same type 
*	The matrix, a two-dimensional array containing values of the same type (the simplest generalisation of an array) 
*	A tensor which is obtained by increasing the dimensions (an n-dimensional array) 
*	An aggregation of several arrays of different types while assigning
a name to each one (This produces a data frame, a data structure available in several programming languages and frameworks [e.g., R, Spark, Python, Mathemathica, Matlab]. It is equivalent to a table in a relational database.) 
  *	The simplest data frame, only named columns (A generalisation ל assigns a name to the rows, as well, and a       more generalised version uses a hierarchical index for rows and/or columns.) 
    Data extracted from databases, or generated by sensors, are examples of these data structures. 
*	Simple graphs, where between two nodes there is either no arc/edge or a single one, the edge has no direction and there are no loops 
*	Directed, where the edge has a direction 
*	Branched (multiple edges between two nodes) 

If there are multiple edges, very often each edge has a label that describes the relation type. The analysis of social networks is based on this data structure. In the field of NLP, the simplest data type is the character, represented in one of several formats (ASCII, UNICODE, using a specific encoding, etc). Based on the level of aggregation, we can have any of the following: 

*	A word which consists of a sequence of characters 
*	A sentence which consists of a sequence of words, separated by spaces (the words) and punctuation marks (the sentences) 
*	A paragraph, a sequence of sentences, separated by full stops, exclamation points or question marks 
*	A section/chapter/document present in a hierarchical organisation (e.g., a book) 
*	A corpus, a list of documents 

The main problem with this format is that it does not have a specific encoding (ASCII, UTF8, ISO8859-1, or Windows-1252) and it is not possible to specify a hierarchical structure. Some alternatives are as follows: 

*	HTML, which can structure the text in well-defined elements 
*	XML, a generalisation of the HTML 
*	Markdown, a lightweight version of the HTML which allows the user to specify a not too complex hierarchical structure 

2.4.2 AI File Formats
~~~~~~~~~~~
The data, used by the algorithms, are saved on files with a structure that depends on the data type. The simplest data format is comma-separated values (CSV), a text file in which: 

*	records are separated by a new line, and 
*	fields are separated by a comma. 

The file can contain, in the first row, the column names. However, this format is not the only standard: 

*	In certain formats, the fields are separated by a tab rather than a comma. 
*	The row containing the column names can be removed. 
*	There is no standard method to represent strings with spaces or commas (or tabs). 
*	There is no standard method to represent missing values. 
*	There are multiple methods to represent date/hour/timestamp. 
*	There is no consensus on whether a CSV file can contain comments or not. 

Another problem is that the column’s data type is not specified. To obtain this information, it is necessary to analyse the records and to use a heuristic approach to find them, or have them passed from the user. A simple variant format is the attribute-relation file format, or ARFF: it is very similar to a CSV, but it contains a header with the name and the type of each column. Other popular text formats are XML and JSON. The main problem with the text formats is that it is necessary to read the file sequentially to read its context. This is a serious problem if the file is huge and it is used in a big data infrastructure (e.g., Hadoop – refer to Section 2.12). 

Some other formats available are as follows: 

*	Adobe PDF 
*	Microsoft Word 
*	OpenXML (proposed by Microsoft) and OpenDocument (proposed by OpenOffice and Sun StarOffice, now IBM): a compressed list of XML files, containing the text, its formatting and the hierarchical organisation 


2.5 Data Representation Standards
-------
2.5.1 Basic ISO Working Groups and Standards
~~~~~~~~~~~
To provide a stable base to address the challenges and opportunities of data management in AI and big data scenarios, a comprehensive range of standards and technical reports has been published by the International Organization for Standardization (ISO). 

* ISO/IEC JTC 1/SC 32, titled ‘Data Management and Interchange’ and currently called ‘WG2 on Metadata Standards’, focuses on three major areas: 
	* Specification of generic classes of data, metadata and frameworks for representing the meaning and syntax of data, including metamodels, ontologies, processes, services and behaviour, plus the mappings between them 
	* Specification of facilities to manage metadata, including registries and repositories 
	* Specification of facilities to enable electronic metadata exchange ל over the internet, within the cloud, and via other information technology telecommunications avenues 

* ISO/IEC JTC 1/SC 42, titled ‘Artificial Intelligence’, deals with data management in AI pipelines. It has published six relevant standards, among which is the five-part ISO/IEC 20547 series, which provides a big data reference architecture (BDRA) organisations can use to effectively and consistently describe their AI-ML life cycle and its implementation. The BDRA addresses requirements, architecture, security and privacy, use cases and considerations that architects, application providers and decision-makers will want to consider in deploying a big data system. The list of published standards includes the following: 

	* ISO/IEC 20546:2019, Information Technology – Big Data – Overview and Vocabulary 
	* ISO/IEC TR 20547-1:2020, Information Technology – Big Data Reference Architecture – Part 1: Framework and Application Process 
	* ISO/IEC TR 20547-2:2018, Information Technology – Big Data Reference Architecture – Part 2: Use Cases and Derived Requirements 
	* ISO/IEC 20547-3:2020, Information technology – Big Data Reference Architecture – Part 3: Reference Architecture 
	* ISO/IEC TR 20547-5:2018, Information Technology – Big Data Reference Architecture – Part 5: Standards Road Map 
	* ISO/IEC TR 24028:2020, Information Technology – Artificial Intelligence – Overview of Trustworthiness in Artificial Intelligence 

Other ISO standards relevant to AI data representation include the following: 

*	ISO/IEC 11179:2019, Metadata Registries (MDR) – A framework for registering and managing metadata about data sets 
*	11179-2:2019, Part 2: Classifications – Describes the registration of classification schemes and using them to classify registered items in a metadata repository. Any metadata item can be made a classifi- able item so it can be classified, including object classes, properties, representations, conceptual domains, value domains, data element concepts and data elements themselves. 
*	11179-3:2013, Part 3: Registry Meta Model and Basic Attributes – Specifies the structure of a metadata registry in the form of a conceptual data model, which includes basic attributes that are required to describe metadata items 
*	11179-3:2019, Part 3: Registry Meta Model – Core Model – Specifies the structure of a metadata registry in the form of a conceptual data model 
*	11179:7:2019, Part 7: Meta Model for Dataset Registration – Provides a specification in which metadata describing data sets, collections of data available for access or download in one or more formats, can be registered 
*	ISO/IECTR19583, Concepts and Usage of Metadata 
*	19583-1, Part 1: Metadata Concepts – Provides the means for understanding the concept of metadata, explains the kind and quality of metadata necessary to describe data and specifies the management of that metadata in an MDR 
*	19583-2, Part 2: Metadata Usage – Describes a framework for the provision of guidance on the implementation and use of the registries specified in ISO/IEC 11179, Information Technology – Metadata Registries, and ISO/IEC 19763, Information Technology – Meta Model Framework for Interoperability (MFI) 
*	ISO/IEC11404:2007, General Purpose Data Types (GPD) – Specifies a collection of data types commonly occurring in programming languages and software interfaces including both primitive and non-primitive data types, in the sense of being wholly or partly different in terms of other data types 

2.5.2 ISO Work Groups and Activities on Data Governance
~~~~~~~~~~~~~
ISO/IEC JTC 1/SC 40, titled ‘IT Service Management and IT Governance’, currently WG1 on Governance Standards, leads the development of standards, tools, frameworks, best practices and related documents on the governance of information technology. Relevant standards potentially beneficial to AI include the following: 

*	ISO/IEC 38505-1:2017, Part 1: Application of ISO/IEC 38500 to the Governance of Data – Applies to governance of the current and future use of data that is created, collected, stored or controlled by IT systems, affects the management processes and decisions relating to data 
*	ISO/IEC 38505-2, Part 2: Implications of ISO/IEC 38505-1 for Data Management – Identifies the information that a governing body requires to evaluate and direct the strategies and policies relating to a data-driven business and the capabilities and potential of measurement systems that can be used to monitor data performance and uses. 

2.6 Representation Standards for Web Data
-------
Web data are at the core of many AI applications revolving around users’ behaviour in cyberspace. Next, we touch upon some of the most well-known representations of the data and metadata designed specifically for web data. 

2.6.1 The Dublin Core
~~~~~~~~~~~
This standard emerged to produce a general metadata standard for describing web pages. Originally created in 1995, Dublin Core (DC) included thirteen elements (attributes) that were later extended to fifteen in 1998 and again, as Qualified DC, to eighteen, including audience, provenance, and rights holder. DC was initially based on text and HTML but evolved to include the concept of namespaces for elements (with approved terms for the semantics of element values) coincident with the move to Quali- fied DC and towards using XML. Later the community realised that relationships among elements were important, and an RDF version was proposed. However, the major volume of DC metadata is still in HTML format and so the benefits of using namespaces – and later relationships – have not been realised. Indeed, this is the major criticism of DC: it lacks referential integrity and functional integrity. The former problem means that it is hard to disambiguate element values in repeating groups. 

2.6.2 Data Catalog Vocabulary (DCAT)
~~~~~~~~~~~
The original DCAT was developed at the Digital Enterprise Research Institute, refined by the eGov Interest Group and then finally standardised in 2014 by the Government Linked Data Working Group, leading to a W3C recommendation. It is based on Dublin Core but adopts linked data principles with a schema including links between a data set and a distribution of that data set (i.e., a replicate or version), a data set and a catalogue and also between a data set and an agent (person or organisation). 

2.6.3 Common European Research Information Format (CERIF)
~~~~~~~~~~~
CERIF is a European Union Recommendation to Member States. CER- IF91 (1987–1990) was quite like the later Dublin Core (late 1990s). CER- IF2000 (1997–1999) used full enhanced entity-relationship (EER) modeling with base entities related by linking entities with role and temporal interval (i.e., decorated first-order logic). In this way, it preserves referential and functional integrity. There are commercial CERIF systems, two of which were bought by Elsevier and Thomson Reuters to include CERIF in their products. 


2.7 Data Representation in Key Vertical Domains
-------
Several vertical domains of interest for AI do not yet have common data representations but have nevertheless started initiatives in data format sharing. 

2.7.1 ISO Activities on Space Data
~~~~~~~
The Consultative Committee for Space Data Systems (CCSDS) was formed in 1982 with the goal of gathering best practices by the major space agencies of the world and developing a common solution to the operation of space data systems. While the CCSDS is concerned primarily with space data, the work of ISO TC20/SC13 is applicable well beyond the space data community. The National Archives and Records Administration and other digital cultural organisations also participate in the group. Much of the work is focused on long-term (long enough to be concerned about obsolescence and usability) preservation and use of information, and interoperability between data repositories, data producers and their users. Relevant standards include the following: 

* ISO 16363, Audit and Certification of Trustworthy Digital Repositories (TDR). The OAIS Reference Model is adopted by many ‘OAIS- compliant’ digital repositories. At the time ISO 14721 was first developed, there was no standard to assess compliance with the reference model. ISO 16363 was developed to fill that gap. In addition to providing for the audit and certification of TDRs, the standard can serve as a road map for developing the policies, procedures, staffing and infrastructure for setting up a TDR that is compliant with the OAIS Reference Model. 

2.8 Data Representation for Bioinformatics
-------
Applied Proteogenomics Learning and Outcomes (APOLLO) aims to correlate all genomic, proteomic and clinical data with imaging data with a focus on precision medicine or targeted medicine. Three major developments were launched. First, in the Precision Oncology Program (POP, March 2015), the US Department of Veterans Affairs (VA) program focused initially on lung cancer. It was designed to seamlessly merge traditional clinical activities with a systematic approach to exploiting potential breakthroughs in genomic medicine and generating credible evidence in real world settings and in real time. The second program, Apollo (July 2016), was inspired by Moonshot, where a coalition was formed between the US-VA, the US Department of Defense and the US National Cancer Institute to help cancer patients by enabling their oncologist to more rapidly and accurately identify drug treatments based on the patient’s unique proteomic profile. The third program was Research POP (RePOP, July 2016), the research arm of POP, consisting of veterans who agreed to share their medical records (clinical, imaging, genomic, etc.) within and outside the VA for the purpose of finding the cure for cancer. The Veterans Health Administration consists of 8,000,000 veterans, 160 VAMC, 800 clinics, 135 nursing homes. It also has the backbone operational infrastructure of the Veterans Information Systems and Technology Architecture (VistA). 

2.9 Data Representation for Smart Cities
-------
Smart cities provide a rich environment with heterogeneous data from many diverse IoT sensors. The complexity of such data collection includes different real-time communication protocols, data formats, data stores and data processing methods, either at the edge or at the central office. The combined data enables decision-making from everyone, from the city residents to the city government. 

2.10 Data Representation for Intelligent Manufacturing
-------
Smart manufacturing plays a central role in data integration, from diverse supply chains of raw materials on product specifications to quality monitoring throughout the production life cycle. Additional data and metadata are generated from many different supporting sensors and machinery for real-time analysis and decision-making to provide safe and healthy environments, bring precise and quality processes and deliver reliable and superior products. 

2.11 Recommendations
-------
Supporting diversified representations for AI data assets is essential for organisations to reduce the corporate burden of AI. Key recommendations include the following: 

*	Utilise standard metadata as much as possible to capture precise descriptions, data types, properties, unit of measurement, characteristics, etc., for given data elements. 
*	Adopt/develop standard metadata registries to support catalogues and types registries. 
*	Adopt/develop standard interfaces to support online data element definition. 
*	Adopt/develop standard computable object workflow functionality to trigger non-functional properties, including privacy and ethical issues in AI-ML data assets. 

2.12 Big Data Systems
-------
Data storage requirements for AI vary widely according to the application. Medical data, as well as imaging data sets used in military applications, frequently combine petabyte-scale storage size with individual files in the gigabyte range. Numerical data used in industrial areas such as maintenance, like the running example in the previous chapter, are often much smaller. 

One of the key requirements of big data storage systems is to handle very large amounts of data and maintain the rates of high input/output operations per second (IOPS) needed to feed some AI-ML models. Indeed, these requirements are incompatible with traditional file system organisation based on files and folders. 

When performance is not the top priority and one can accept response times on the order of seconds, scale-out (or clustered) network-attached storage (NAS) can be used. NAS consists of file access shared storage that uses parallel file systems distributed across many storage nodes to handle billions of files without the kind of performance degradation that occurs with ordinary file systems as the folder tree grows. Another storage technology that can handle very large numbers of files is object storage. This tackles the same challenge as NAS – traditional tree-like file systems become unwieldy when they contain too many files. Object-based storage copes with this issue by giving each file a unique identifier and indexing the data and their location. Object storage systems can scale to very high capacity and large numbers of files estimated to be in the billions. Flash storage is commonplace now, while NVMe flash is emerging as the medium of choice for applications that require the fastest access for data stored near the graphics processing unit (GPU). The spinning disk is still there, too, but is increasingly being relegated to bulk storage on lower tiers. 

2.12.1 Big Data Platform Structure
~~~~~~~
Big data platforms are software systems designed to process huge amounts of data in a short time. We can classify these platforms according to their data ingestion modalities (Section 1.3.3): 

#. Platforms for stream data 
#. Platforms for batch (stored) data 

An example of the first category of platforms is the software platform used by Twitter to monitor in real time the messages sent and received; the second category includes the EOSDA cloud-based platform to analyse satellite imagery for business and science purposes.: 
Big data platforms rely on the following principles: 

#. Distribution: A single high-performance computer is not sufficient to handle the AI-ML workload. Thus, big data platforms use clusters of low-cost machines connected together. 
#. Data chunking: Data are split into smaller chunks that can be processed independently. 
#. Parallelism: Each task of the life cycle is subdivided into smaller tasks that can be executed in parallel 

However, the existence of a high number of nodes introduces another problem: an increased probability that some nodes can crash or the storage system can fail. To overcome these problems, big data platforms use three main strategies: 

#. Functional paradigm. The implementation of the tasks follows the functional programming paradigm. The main property of this paradigm is that it has no side effects: different nodes, executing the same task on the same data, in different instants, generate the same result, regardless of which tasks have been previously executed. 
#. Replication. The data are replicated several times: in this way, if a storage system’s component fails, there exists another copy available somewhere else. 
#. Graceful failure. If a task fails, the same task can be submitted to another node, or, in the real-time systems, the same task can be executed two or more times, and the result can be obtained from the working node. 

2.12.2 Hardware Issues
~~~~~~~
Besides the software architecture, AI-ML models’ need for speed has encouraged the use of a high number of GPU-intensive clusters. GPUs were originally used to accelerate memory-intensive geometric calculations such as the rotation and translation of polygons’ vertices into different coordinate systems. 
Since most of these computations involve matrix and vector operations, GPUs have become increasingly used for non-graphical calculations; they are especially suited to parallel problems. AI-ML models’ requirements boosted the interest of GPUs. While training AI-ML models, GPUs can be hundreds of times faster than Central Processing Units (CPU)s. Today, there is some competition between GPU and custom integrated circuits (ASICs), including the tensor processing unit (TPU) designed by Google. 

2.12.3 File Formats
~~~~~~~
Some file formats for big data are more efficient than regular formats for AI-ML applications, as they permit to read only specific parts of the file. The most famous one, now a standard, is Hierarchical Data Format, versions 4 and 5). It is able to save in an efficient way (in binary and compressed format) a data frame with hierarchical indices where the values can be matrices or tensor data used by AI-ML models. 

Other specialised data formats include the following: 

*	Apache Parquet. A columnar data structure, defined by Cloudera and Twitter 
*	Apache ORC (Optimized Row Columnar). Defined by Hortonworks and Facebook 
