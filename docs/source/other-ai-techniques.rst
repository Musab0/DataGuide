9. Other AI Techniques
===========

Although very general, the AI-ML life cycle discussed in this document covers only a part (i.e., supervised ML) of the rich landscape of AI applications. Indeed, not all AI applications ultimately require ML. Historically, some AI techniques preceded it. When researchers started to investigate AI, they were focused on the so-called symbolic techniques like automated inference and reasoning; ML had not even occurred to them. An example of the use of AI without ML are rule-based expert systems. Human-defined rules allow expert systems to perform inferences to a limited extent. Another example is syntax-based NLP, where the syntax and semantics of natural language are encoded into algorithms (parsers) used to interpret and generate language. In 1985, this type of AI received the name of Good Old-Fashioned Artificial Intelligence (GOFAI). In many cases, symbolic reasoning is sufficient, especially if a large amount of domain knowledge is made available in the form of rules. Another advantage of using the symbolic form of AI instead of ML or DL is that there is no black box problem. As we discussed in Section 1.7, ML-based decision-making may lack transparency, and bias hidden in the training data may lead to unfair decisions. Besides symbolic techniques, other areas of AI rely on statistical or algorithmic techniques rather than on ML. In this section we quickly review some non-symbolic AI techniques like time series forecasting and optimisation that do not rely on ML but still need sound data management practices. Our introduction will not cover other important meta-heuristic techniques that come under optimisation, such as evolutionary algorithms, tabu search, simulated annealing or swarm intelligence. The interested reader should consult the technical literature. 

9.1 Predicting Trends from Data
----------
Many natural phenomena vary with time in a non-trivial way. Applications may need to find out the behaviour of such phenomena in the short or medium term. This kind of analysis is known as time series forecasting. For instance, one may want to predict, with some approximation, the trend of the air cargo market over the next six months. It is possible to address this problem by observing the behaviour of the phenomenon in the past: if its behaviour in a specific time period is correlated to the behaviour of the following time period, then this insight can be used for prediction. For instance, looking at the water level of the River Nile across time has been used for centuries to predict forthcoming floods and droughts. Moreover, different phenomena are often correlated to one another: one can observe, for instance, that the trend of the cargo market follows the trend of the global economy with just a few months of delay. Thus, the global economy can be used to predict the behaviour of the cargo market. This is an example of how knowledge of correlations among different time series can help forecasting. The difficult part in forecasting in the mentioned examples is the discovery of significant enough correlations that are hidden in the data. Noisy data are particularly difficult to deal with. Sifting through large volumes of data in search for regularities thus requires powerful data-mining techniques. There are multiple models and methods used as approaches for time series forecasting. The simplest series forecasting setting is the univariate time series forecasting problem, where data contain only two variables: time and the value to be forecast. In the multivariate time series forecasting method, forecasting problems contain multiple variables, one of which is again time. Modelling techniques include the following: 

* ARIMA model. ARIMA is a combination of three different models, AR, MA and I, where ‘AR’ reflects that the evolving variable of interest is regressed on its own prior values, ‘MA’ states that the regression error is the linear combination of error term values from preceding times, and ‘I’ expresses the fact that data values are replaced by the difference between themselves and the previous values. 
* Autoregressive conditional heteroscedasticity (ARCH) model: The ARCH model is the most volatile model for time series forecasting, capable of catching dynamic variations of volatility from time series. 
* Autoregressive model or VAR: This model computes the independencies between various time series data as a generalisation of the univariate autoregression model. 

From the data assets’ point of view, time series analysis requires data ingestion at regular intervals, which is best implemented via stream platforms for big data. 


9.2 Retrieving the Right Information from Large Repositories
----------
Due to the astronomical growth in data generation and the move towards open data initiatives, there is now an enormous number of data sources available to choose from. However, due to the myriad of choices, it becomes impossible to select what is relevant to the problem at hand. For instance, the internet consists of billions of pages of information, and it would be impossible to find the exact information you are looking for without the help of search engines to rank the pages in order of relevance. 

Those engines use the links pointing from one page to another: they are based on the premise that the pages with the most references from highly referenced pages are the most relevant. Ranking algorithms of this kind can be used to retrieve, by relevance, documents from any repository and to extract value from the information, instead of drowning in the deluge of data. Information retrieval techniques can be used with all kinds of data repositories: text documents, images and videos. 

9.3 AI Optimisation and Data
----------
Optimisation is an important branch of AI that is often overlooked. It revolves around making optimal decisions, and data play an important role in that. We make decisions every day: for example, choosing the best route to commute, buying better-value products or investing to get maximum returns. This is also the case for businesses and large organisations. A logistics business with a large fleet of vehicles, for instance, needs to make a daily decision about optimal vehicle routing; optimisation in this setting means minimising travel time and maximising the number of deliveries. Sub-optimal routing may result in unnecessary travel and poor customer service. Similarly, in teleco, choosing the best sites to deploy mobile cells is important. Deploying mobile cells in less-ideal locations can have negative cost and revenue implications. This concept generalises to many other sectors and businesses. Logistics, hospitality, airlines, oil and gas, health, education, and government – they all have to make the best possible decisions to run their organisations efficiently, provide the best service to their customers, minimise costs and maximise benefits. In the example of a logistics business, it may be trivial to find the best routes for an organisation with only one or two vehicles. However, with a larger fleet, the possible combination of routes becomes infinitely large, and the routing complexity increases: the decision about where to route one vehicle can depend on other vehicles’ potential routes, making it impossible to utilise a manual routing process to find the best options. AI optimisation intelligently searches through the complex set of options and provides optimal (or near-optimal) decisions to implement. 

As we have seen in the previous chapters, data availability is an important factor in decision-making and should be given careful consideration when formulating an optimisation model, as well. There are three main components of an optimisation model: 

#. Decision variables 
#. Objectives
#. Constraints 

In our example of the logistics business, decision variables define a set of routes that have to be optimised. The two potential objectives are to minimise total travel and maximise delivery volume. And the potential constraints are the capacity of each vehicle and their hours of operation. A clear input data about the domain is crucial to define each of these components. For example, location data about the customers as well as street-level distances are required to specify the routes. Real-time or historical data about traffic and any ongoing construction works could also be useful to calculate the expected travel. 

Finally, data about the vehicles’ size and speed as well as drivers’ roster data are required to calculate the constraints involved in optimising routes. In many real-world scenarios, data are created with the mindset that they will be interpreted by humans, often lacking the required details and rigorous- ness to be processed automatically. To capitalise on the data for AI optimisation, it should be carefully created to enhance the entire decision-making process. 

