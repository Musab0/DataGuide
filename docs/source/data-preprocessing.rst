4. Data Preprocessing
=========
The term data preparation is used to designate the first step of the preprocessing phase, where raw data are improved to make them fit for AI-ML applications. It typically consists in determining the data quality to execute appropriate data cleansing procedures. Depending on the application, data cleansing may be limited to the elimination of individual or systematic errors or inconsistencies with the data types or proceed further to enable or improve the efficiency of the data application process. The latter may also involve filling in missing data, encoding the data to the less occupant data types or to meet the numerical format requirements or even, in case of supervised ML applications, to label or tag the data samples into application-defined data patterns or classes.

4.1 Quality and Cleanness of Data
---------
In general terms, data quality or cleanness refers to the level of data completeness, consistency, accuracy or precision defined by the data requirements and variable descriptions or implied from the distribution of values. This sort of independent interpretation of the data quality is often extended in a case of known data utility or application to simply inform, to which extent the data in its current state can be readily used to solve the specific problem. In all the cases, the general rule is that the better the quality of the data, the less preprocessing/cleaning effort is required and the sooner they can be utilised to generate better data application outcomes. 

Several standard data quality characteristics allow for rapid evaluation of the level of preprocessing effort needed to clean the data and often inform the choice of the appropriate data modelling technique for the data at hand. These data quality characteristics are as follows: 

#.	Data completeness describes the degree to which each field contains a complete or non-missing set of values as defined by the data size. It is usually measured in percentage terms for each data variable/field. It is worth noting that missing data may or may not be specifically marked in the data. Although white space or the markers NULL, NAN, NIL are often used to mark the missing data entry, in general one should receive the missing data marker from the data provider or otherwise infer it from the data. 
#.	Data consistency refers to the level of agreement in format and type that is observed within the data set or its variables/fields. For example, if in a date column full of dates in dd/mm/yyyy format, we suddenly observe the number 45 or the colour red, this is clearly a value that is inconsistent with the date type. In fact, even the value 13-MAR-2020 would be inconsistent because it uses a different date format. For irreconcilable data inconsistency (as in the first case above), the marker must be replaced with the missing value marker; in the case of a minor format inconsistency (as in the second case above), it must be corrected to the correct form. 
#.	Data accuracy refers to the degree of correctness with which data values reflect the measured characteristics. The assumption here is that any inaccurate data are correct in terms of the type and format required by the variable they describe, but their values are simply wrong. Data inaccuracies can be a result of flaws in the measurement theory, mistakes in the data acquisition process or various other possibly compounded errors unaccounted for in data generation, which only surface during a quality check in the form of spikes, outliers, distribution perturbations and other statistical inconsistencies. 
#.	Data precision refers to the required level of detail to sufficiently describe a specific variable with an instance of a measurement. If, for instance, a floating-point temperature measurement with three decimal digits’ precision is suddenly met with integer values, or if a millisecond’s precision timestamps are mixed with daily dates, we consider such variable instances imprecise or of variable precision. Low-precision data are considered to be a less severe data quality issue, as they do not lead to data removal but simply to the attribution of certain non-zero uncertainty bounds around the precise measurements. 



4.2 Data Cleansing
---------

Data cleansing covers all the preprocessing activities that are required to be carried out on the data to eliminate or resolve data quality issues. As mentioned before, these activities can be divided into a standard set of activities that are application independent, which improve the state of the data whatever the application, and activities that are dependent on/guided by the data application to efficiently and effectively improve data utility for this application. The boundary between the two cleansing techniques, application dependent and independent, is often blurred. The reason for the reduced distinction is that there are a growing number of possible applications of data; therefore it is increasingly easier to find at least one way to subjectively enhance the improvement by performing the ‘standard data cleansing‘ differently. For the same reason, it is always advisable to keep a copy of uncleansed raw data as a reference backup point that the data can be reverted back to if needed. 

The data quality issues that can safely be cleaned whatever application is used consist of removing data inconsistencies, both by marking the inconsistencies as missing values and correcting the inconsistent formats to the correct form. The data imprecisions are often simply left in the same state or are artificially transformed to the dominant precision if possible. 

The cleansing of all other data quality issues is mostly dependent on the data application. There are two data cleansing practices that are most often carried out in preparation for AI applications: 

#. Filling in missing data is typically required by supervised and unsupervised ML but also by many statistical analysis methods that are saturated with mathematical methods and numerical processing techniques. Not filling in the missing values would result in infinities, singularities or other errors. There are a number of standard methods for filling in miss- ing data, and their effectiveness depends on the context and type of data application. They may also be considered in terms of the risk level associated with the introduction of new values of uncertain validity. 
It is important to remark that filling in a missing data point is not a necessity in all circumstances, especially in the first stages of the life cycle. In fact, the most risk-averse or agnostic approach to missing data handling is to simply clean and replace it with the single representation that some methods may be able to use. For example, a number of mathematical models can handle infinities, as well as so-called not-a-number (NaN) entries. Moreover, in case of categorical variables, missing values can simply be pulled together and labelled as a new missing value: NULL. In both cases, filling in missing values really becomes an exercise in replacing all their different forms with the single representation accepted by the application and consistent with the specification that can handle them (i.e., NaN or NULL). Following this approach eliminates the risk of introducing new uncertainties along with the missing data. 
One of the most conservative methods of filling in missing values is inserting the variable’s mean, mode or median value, whichever is most appropriate. In the case of temporal or sequential data, the least risky method for filling in the missing values is the copydown method, which simply populates the missing values with the last known non-missing value in a sequence. In case the sequence starts with some missing values, the next least risky strategy is to fill in this initial missing section with the first following non-missing value found in a sequence. 
The riskier methods for filling in missing data in the context of multi-dimensional data are based on the concept of data similarity (i.e., the nearest neighbour). Assuming that the data instance is not missing in all dimensions, this method will search for the most similar cases among these other non-missing dimensions. Then, when the target variable value is found, it is simply filled in based on the values from the single most similar neighbour or the average over the k-most similar neighbours. In the sequential data, such a method can be directly replicated with some degree of influence by available temporal constraints such as data interpolation, which ensure the continuous flow of a sequential signal. However, interpolation can be very damaging if the data application uses time series forecasting because it would result in illegally passing future information back to the past. There are, in fact, spectacular failures reported in financial algorithmic trading based on interpolated financial time series. Hence, as mentioned above, the most suitable choice here very much depends on the data application. 
In addition to the most and least risky strategies for filling in missing data, there are others that try to exploit and preserve other data characteristics, typically higher order statistics such as data distribution and data domain bounds, and apply semi-random data sampling from the distributions obtained from the non-missing part of the data examples. These methods address the missing data problem while preserving marginal data distributions, but they also risk injecting accuracy errors by ignoring conditional dependencies with other variables. 
#. Outlier removal is considered a removal of the data values that are significantly different from other observations. Even identification of outliers is typically arbitrary, because unless we possess prior knowledge about the data generation and its constraints (which is rarely the case), it is impossible to determine if the anomalous value genuinely reflects a valid measurement or is the result of some kind of error. Statistical hypothesis tests may be used here to identify the outliers and generate a likelihood value on how probable it is that the value could have been generated from the measured probability distribution, but a more pragmatic approach relies on the data application to decide where to set the threshold for outlier removal so that the performance of the data application is maximised.

In practical terms, outlier removal follows a phase of outlier identification, followed by replacing them with missing data markers and optionally further refilling them using the most suitable method for filling in missing data for the data application at hand. 



4.3 Data Normalisation
---------
The term normalisation informally designates all transformations per- formed on ingested data at the data exploration stage or (more frequently) at the data preprocessing stage of the AI life cycle. It is also possible that the outputs of certain AI-ML models are renormalised before feeding them to other models in scenarios where multiple ML models are connected in a sequence. The overall goal of data normalisation is to bring the values of the components of data vectors to a common scale, without distorting the differences in the value ranges. Normalisation prevents the scales of dimensions in the input data vectors from affecting the dimension’s importance for the AI model that will be used. For example, consider a bi-dimensional data set containing only two features, :math:`v = (V_1, V_2)`, and assume that :math:`V_1` values range 0 ̆10, while :math:`V_2` ranges 0 ̆100000. In some data analysis techniques, like linear regression, :math:`V_2` will end up influencing the regression model's result more than :math:`V_1` due to its larger size, although it may be less useful than :math:`V_1` for computing an accurate output. 

Normalising data also allows designers to neglect the measurement units, enabling AI models to consider all dimensions of a data vector as pure numbers on the same scale. 

It is important to remark that not all data sets require normalisation before applying ML models. It is mandatory only when the features have different ranges. Popular ways to normalise data include the following: 

* Normalising data distribution’s moments. Transforming normally distributed data to obtain a distribution whose mean μ= 0 and standard deviation σ = 1 
* Standardising data values. Transforming data using a z-score. This transformation is usually called standardisation in statistics textbooks. To perform this transformation, one starts from the mean μ and also the standard deviation σ of the data probability distribution. In the AI life cycle, these parameters are estimated in the fitting procedure of the exploration phase. The z-score substitutes each data value with its distance from the mean of the population distribution. This distance is expressed as the number of standard deviations that separate the data point from the population mean. By definition, z-scores are symmetric and follow a normal distribution. They usually span the six sigma interval: from 3 standard deviations (at the far left of the normal distribution curve) to +3 standard deviations (at the far right of the normal distribution curve). Z-scores are also a way to compare normally distributed data values belonging to huge data sets. Estimating how far a given value V lies from the mean may be difficult when V belongs to a data set with millions of entries; the Z-score of V expressed directly where V lies with respect to the population’s mean. 
* Rescaling data values. Transforming data so that they all have values between 0 and 1. This transformation is also called feature scaling. While feature scaling can in principle be computed by dividing all data values by the largest one ( V ), this may bring rounding errors when Vmax Vmax is much larger than the other data values. The most popular rescaling formula used by statistics tools and libraries is 
* Normalizing data vectors: Divide each data vector v by its norm, so all data vectors have a length of one. 


4.4 Data Encoding
---------
In the context of AI-ML models, the term data encoding refers to reversible transformations upon the data vectors’ components or features that map their original values to a new set of discrete values in order to achieve certain benefits for the data application, such as improved model performance. Data encoding offers a way to convert categorical variables to numerical representations such that the AI-ML models exclusively working on numerical data could include additional variables and often deliver improved performance. 

The common strategy that underpins all data encoding methods used in the context of AI-ML is mapping inputs into a discrete (typically ordinal) representation. The most common starting point in data encoding discussions is categorical data encoding. We will also cover numerical and text data encoding while mentioning image and sound data encoding techniques. 

4.4.1 Categorical (Nominal) Data Encoding
~~~~~~~~~~~
Nominal data encoding is aimed at mapping each unique category of the categorical variable into a discrete numerical format that can be easily absorbed by the numerical algorithms of ML models. 

# Ordinal. This encoding simply maps each category of the nominal variable to a discrete number starting from 1. Since the categories are orderless by nature, the order of the encoded categories could be random, but it is a good practise to follow lexicographic order when categories are expressed using alphanumeric symbols. In case of categories typically represented by text, ordinal encoding delivers enormous savings in memory storage requirements because hundreds of bytes required to store one text string entry are replaced by 1 or a handful of bytes to represent the ordinal number. 
# One-hot. Even though categorical values do not have any intrinsic order, ordinal encoding introduces it because of ordinal enumeration, which can mislead certain ML methods which would utilise the numerical distance. The technique of one-hot encoding eliminates this problem by marking the occurrence of each unique categorical value in a separate binary indicator variable. Given m categories of the original categorical variable, one-hot encoding converts them to m binary variables, each exclusively marking the occurrence of the corresponding category. One-hot encoding is suitable to numerically represent categorical variables taking a small number of unique values. Otherwise, the benefits of mapping to orderless numerical representation are outweighed by the growing dimensionality and increased computational cost of processing such data. 


4.5 Data Anonymisation
---------
Anonymisation of the data can be done as part of preprocessing the data or as part of data ingestion. The position of this step in the AI life cycle can be related to the data governance policies at the organisation adopting the AI-ML model. The best approach is often to anonymise in the data ingestion phase, to avoid any data leakage or violation of privacy during the AI-ML life cycle. In the case where data preprocessing is the responsibility of the data owner, it is not unusual for anonymisation to be part of preprocessing. Organisations should be careful in setting up their strategy for anonymisation and in positioning it correctly in the life cycle of their AI application, as will be discussed in the remaining sections. 

4.6 Data Labelling
---------
The classification process associates a label to input data vector. For instance, in health care, an image representing an MRI scan could be associated to a label corresponding to the description of its content: a picture of a skin lesion could be labelled with the diagnosis while labels of chest X-rays could indicate pneumonia. In a sentiment analysis setting, an X post could be associated to a label describing the mood or sentiment of the author with respect to the X post’s subject. Similarly, regression requires a numerical value to be associated to each input: the row of values describing details about a real estate property should have a price associated to it. Labels come in many forms. Prelabelled data sets (open or proprietary) are sometimes available. In other cases, only unlabelled data sets are available, and labelling must be carried out during the AI-ML model development. 

When some or all of the labels are missing, one has to devise strategies to fill in the gaps. The main strategies are the following: 

*	Manual labelling by one or more experts 
*	Labelling by an ‘oracle’ algorithm 
*	Use of unsupervised techniques for partitioning followed by labelling block representatives 

Other strategies available, when the data are partially labelled, involve adopting special learning algorithms in the learning phase: semi-supervised algorithms such as self-training and co-training algorithms.

**Manual labelling**. In manual labelling, human experts take individual examples and attach to them a label taken from a predefined set. This process is typically time consuming, rather expensive and cannot scale to a large number of examples. Manual labelling can either be done in-house, get crowd-sourced or be outsourced to individuals or companies. 

**Oracle algorithms**. The use of an oracle consists of running an available classification algorithm and tagging the data with the labels it issues. Then, the labelled data can be used for training another ML model. The use of or- acle algorithms is relatively rare and justified only in special circumstances because if a well-performing classification model is available, then it is hard to justify spending time and resources on preparing the data and training a new one. One such example is if the available classification algorithm is a black box, while one would like to acquire some understanding of the classification logic; sometimes the available model is known in detail but the architecture is not satisfactory for some reason; for example, the oracle algorithm is available for a limited time or under constraints that cannot be fulfilled in the long run. 
Unsupervised partitioning. In certain scenarios, the data can contain regu- larities that allow for partitioning them into categories based on similarity, yet each partition would be of unknown meaning. The meaning of each partition can be found by human experts at a later stage, which would cut down the labelling effort to simply labelling the partitions rather than the entire data set. This fact can be exploited as follows: the partitioning algorithm is executed on the data (e.g., a clustering algorithm); the output is disjoint sets/blocks of data, each of which can be interpreted as an unknown category; a few examples from each set are labelled manually and the label is extended to all the elements of the set. 

**Semi-supervised algorithms**. The use of semi-supervised algorithms is possible when only part of the data is labelled. This corresponds to the choice of tagging the unlabelled data as part of the learning phase instead of the preprocessing phase. One of the simplest examples of a semi-supervised learning algorithm is self-training. Let us consider, as usual, classification. In self-training, one has a base classifier model (e.g., a Naive Bayesian classifier) which, once trained, can provide for each example both a label and a measure of confidence in that label. The base classifier is trained on the originally labelled data; then it is used to infer the labels of a part of the originally unlabelled data; those data which are labelled with high confidence are selected and added to the original training set, and then the base classifier is retrained on this extended training set and subsequently used to tag new unlabelled data. The process can be repeated until all the data have been tagged and the base model is trained on the largest possible confidently labelled data set. 

4.6.1 Creating labeled data using ML 
~~~~~~~~~~~~~~
While semi-supervised algorithms can deal with data that are partially labelled, sometimes no labelled data can be found at all, and manual labelling could be infeasible, time-consuming or too expensive. To cope with this challenge, a clustering-based labelling approach can be used to create samples of labelled examples. For every category, a cluster of data points should be formed. Then, all samples will be automatical- ly labelled with the same labels as their corresponding clusters. Then, these labelled examples are refined by taking only a subset of examples that contribute to the goodness of the labelling task. In a second phase, the labelled examples are used to train a supervised ML classifier. The classifier will learn the mapping between data points and the categories and then map a new set of unlabelled data points to the corresponding categories. 

The goal is to create labelled examples to feed the supervised ML classifier. The input samples are unlabelled data points. To provide labels for these samples, they are clustered into different clusters representing the different categories. The approach is not strictly constrained to a particular clustering algorithm. Different clustering algorithms can be used, depending on the particular application setting considered. For example, k-prototypes can be utilised to cluster multivariate time series sequences with numerical and categorical attributes, while k-medoids is applied to cluster samples with categorical attributes. The ‘orthogonality’ of the clustering algorithm should be intended as another amenity of the proposed solution. Each sample within one cluster will be labelled with its cluster label. Specifically, to label the clusters, the domain expert should label only the cluster centres based on the known categories. Then the rest of the samples in a cluster will automatically take the same label as their centre.

4.7 Feature Selection
---------
Feature selection is the process of reducing the number of dimensions in the data vectors, before feeding them to the selected AI model. The purpose of this process is eliminating non-informative dimensions to decrease the cost of the AI model computation and, in some cases, to increase the execution speed. There are multiple popular feature selection techniques, we focus on the feature selection techniques that are more popular in the AI-ML domain, starting from filter-based feature selection methods. Filter-based techniques simply discard dimensions, to discuss more sophisticated techniques that couple selections with transformations similar to the ones discussed in Section 3. From the data management point of view, selecting features always involves projecting the highly multidimensional data space built at ingestion time to a subspace with a smaller number of dimensions – in other words, creating views on the data space that correspond to training data sets to be used for specific ML models. 

4.7.1 Dimensionality Reduction
~~~~~~~~~~
Feature selection is the most straightforward way of reducing dimensionality. However, there are more sophisticated techniques for reducing the number of input variables: they operate not just as a selection method for variables but also as a transformation. Among the categories of dimensionality reduction techniques one can distinguish linear methods, such as the matrix factorisation techniques, from non-linear methods, such as manifold learning techniques or more recent techniques based on auto-encoders. Matrix factorisation methods can be used to reduce the data set matrix (whose rows are the individual data vectors) into its constituent parts. Examples include the eigen-decom- position and the singular value decomposition. The most important dimensions (e.g., those with largest eigenvalues) are kept, whereas the less important are discarded. The most common matrix factorisation technique for ranking the components is PCA. 

4.7.2 Principal Component Analysis
~~~~~~~~~~
To reduce data sets’ dimensionality while preserving the information in the data, PCA computes a data set composed of new, uncorrelated variables that: 

# are linear functions of those in the original data set, and 
# maximise variance 

A set of p original variables can be replaced by an optimal set of q < p derived variables which are named principal components. While PCA (and other dimensionality reduction techniques) are usually performed after the preprocessing stage of the AI-ML life cycle, subspaces with q = 2 or q = 3 are sometimes used at the data exploration stage to obtain a visual representation of a multidimensional data set. Many variants of the PCA technique have been developed to handle different data types. Although a normal distribution of the data set is usually assumed, PCA does not, in principle, need any distribution assumptions and, as such, is very much an adaptive exploratory method that can be used on numerical data of various types. 

PCA can be based on either the covariance or the correlation matrix of the original data. To understand the covariance-based computation of PCA, let us consider a data set with n data vectors x1, . . . , xp, each composed of p numerical variables. These data values define a number p of n-dimensional vectors or, equivalently, an nÖp matrix X. Covariance-based PCA computes a linear combination of the columns of X with maximum variance. For the more mathematically aware reader, we recall that such linear combinations can be computed by multiplying X to a vector a = a1, a2, . . . , ap. The variance of such a linear combination is given by var(Xa) = aT Sa, where S is the covariance matrix associated with the data set and aT is the transpose of a. To identify the linear combination Xa with maximum variance, it is sufficient to compute the p-dimen- sional vector a that maximises aT Sa. Algebra tells us that for this linear system to have a well-defined solution, an additional restriction must be imposed. The most common restriction is requiring aT a = 1. This problem is equivalent to maximising f (a) = aT Saλ(aT a1), where λ is a Lagrange multiplier. The maximum corresponds to a point where the derivative of f(a), which is denoted as f i(a) and expresses the slope of f(a), is zero (see Chapter 6). The condition fi(a) = 0 tells us that vector a that maximises aT Sa must be a (unit-norm) eigenvector of the covariance matrix S, and λ must be the corresponding eigenvalue. Any p p symmetric matrix such as S has exactly p real eigenvalues, and PCA selects the largest one, λ1 (and its corresponding eigenvector a1). The procedure is iterated for all the eigenvalues λk, (k = 1, . . . , p), which are the variances of the linear combinations, and the corresponding eigenvectors ak, which form an orthonormal set of vectors that is the basis for the PCA-selected sub- space, whose axes are called the principal components of the data set. 

4.7.3 Other Techniques
~~~~~~~~~~~~
Manifold learning is used to create a low-dimensional projection of high-dimensional data, a process that – like PCA – can also be exploited for data visualisation. The projection is designed to create a low-dimensional representation of the data set whilst best preserving the salient structure or relationships in the data. Examples of manifold learning techniques include Kohonen self-organising maps (SOM), Sammon mapping, multidimensional scaling (MDS), and t-distributed Stochastic Neighbor Embedding (t-SNE). 

